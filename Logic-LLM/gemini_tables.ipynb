{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./baselines/evaluation/evaluation_baselines.json\", \"r\") as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "with open(\"./outputs/evaluation/evaluation_logic_programs_gemini.json\", \"r\") as f:\n",
    "    logic_lm_results = json.load(f)\n",
    "\n",
    "\n",
    "dataset_names = (\"ProntoQA\", \"ProofWriter\", \"FOLIO\", \"LogicalDeduction\", \"AR-LSAT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Logic_LM_table(logic_lm_results, dataset_names, backup='CoT'):\n",
    "    for dataset in dataset_names:\n",
    "\n",
    "        table_header = f\"|{dataset}|\"\n",
    "        table_header += \"|\".join(logic_lm_results.keys()) \n",
    "        table_header += \"|\\n |---|---|---|---|---| \\n\"\n",
    "\n",
    "        table_rows = []\n",
    "        for metric in ['Overall_Accuracy', 'Executable_Rate', 'Executable_Accuracy']:\n",
    "            row = f\"|{metric}|\"\n",
    "            for model_name in logic_lm_results.keys():\n",
    "                try:\n",
    "                    row += f\"{100 * logic_lm_results[model_name][dataset][backup][metric]:.2f}\"\n",
    "                    if logic_lm_results[model_name][dataset][backup]['Overall_Accuracy'] == 0:\n",
    "                        print(model_name, dataset, backup)\n",
    "                except:\n",
    "                    row += \"---\"\n",
    "                row += \"|\"\n",
    "            table_rows.append(row)\n",
    "\n",
    "        table_string = table_header + \"\\n\".join(table_rows)\n",
    "        print(table_string)\n",
    "        print()\n",
    "\n",
    "print_Logic_LM_table(logic_lm_results, dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.96 | 99.34 | 97.40\n",
      "gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409   CoT\n",
      "56.67 | 74.11 | 79.73\n",
      "gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0409   CoT\n",
      "66.67 | 77.61 | 82.67\n",
      "gemini-1.5-flash-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0409   CoT\n",
      "59.00 | 69.00 | 84.67\n",
      "gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0514   random\n",
      "28.14 | 25.54 | 38.53\n",
      "gemini-1.5-pro-preview-0514 | gemini-1.0-pro-vision-001 | gemini-1.5-pro-preview-0514   Direct\n"
     ]
    }
   ],
   "source": [
    "def print_best_vs_best_table(baseline_results, logic_lm_results, dataset_names):\n",
    "    for dataset in dataset_names:\n",
    "        max = -1\n",
    "        base_direct = -1\n",
    "        base_cot = -1\n",
    "        \n",
    "        max_model = \"\"\n",
    "        base_direct_model = \"\"\n",
    "        base_cot_model = \"\"\n",
    "\n",
    "        for model_name in baseline_results.keys():\n",
    "            for backup in [\"random\", \"Direct\", \"CoT\"]:\n",
    "                acc = logic_lm_results[model_name][dataset][backup]['Overall_Accuracy']\n",
    "                if acc > max:\n",
    "                    max = acc\n",
    "                    max_model = f\"{model_name}   {backup}\"\n",
    "            direct_acc = baseline_results[model_name][dataset]['Direct']['Average_EM_score']\n",
    "            cot_acc = baseline_results[model_name][dataset]['CoT']['Average_EM_score']\n",
    "            if direct_acc > base_direct:\n",
    "                base_direct = direct_acc\n",
    "                base_direct_model = model_name\n",
    "            if cot_acc > base_cot:\n",
    "                base_cot = cot_acc\n",
    "                base_cot_model = model_name\n",
    "\n",
    "        row_acc = \" | \".join([f\"{100*base_direct:.2f}\", f\"{100*base_cot:.2f}\", f\"{100*max:.2f}\"])\n",
    "        row_model = \" | \".join([base_direct_model, base_cot_model, max_model])\n",
    "\n",
    "        print(row_acc)        \n",
    "        print(row_model)\n",
    "        \n",
    "print_best_vs_best_table(baseline_results, logic_lm_results, dataset_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset | info | Direct | CoT| Logic-LM (backup) |\n",
    "| --- | --- | --- | ---  | --- | \n",
    "|  ProntoQA | best acc |  79.96 | 99.34 | 97.40\n",
    "|  ProntoQA | best model | gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409   (CoT)|\n",
    "|  ProofWriter | best acc |  56.67 | 74.11 | 79.73|\n",
    "|  ProofWriter | best model |gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0409   (CoT)|\n",
    "|  FOLIO | best acc |  66.67 | 77.61 | 82.67|\n",
    "|  FOLIO | best model |gemini-1.5-flash-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0409   (CoT)|\n",
    "|  LogicalDeduction | best acc |  59.00 | 69.00 | 84.67|\n",
    "|  LogicalDeduction | best model |gemini-1.5-pro-preview-0514 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0514   (random)|\n",
    "|  AR-LSAT | best acc |  28.14 | 25.54 | 38.53|\n",
    "|  AR-LSAT | best model |gemini-1.5-pro-preview-0514 | gemini-1.0-pro-vision-001 | gemini-1.5-pro-preview-0514   (Direct)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.60 | 47.40 | 79.96 | 63.80 |\n",
      "84.47 | 98.51 | 99.34 | 92.55 |\n",
      "77.40 | 95.60 | 46.40 | 46.94 |\n",
      "77.40 | 96.20 | 80.00 | 62.50 |\n",
      "77.40 | 97.40 | 95.00 | 94.39 |\n",
      "\n",
      "34.83 | 15.17 | 56.67 | 53.83 |\n",
      "56.67 | 74.11 | 59.83 | 66.84 |\n",
      "61.27 | 74.66 | 34.50 | 32.17 |\n",
      "55.43 | 76.01 | 56.67 | 53.67 |\n",
      "69.28 | 79.73 | 64.17 | 67.17 |\n",
      "\n",
      "59.80 | 39.71 | 40.20 | 66.67 |\n",
      "63.78 | 77.61 | 48.51 | 67.34 |\n",
      "53.00 | 62.38 | 71.14 | 36.27 |\n",
      "65.50 | 72.77 | 81.59 | 68.14 |\n",
      "67.50 | 82.67 | 81.59 | 68.63 |\n",
      "\n",
      "45.67 | 53.33 | 59.00 | 54.67 |\n",
      "57.67 | 69.00 | 60.20 | 55.33 |\n",
      "61.33 | 60.33 | 84.67 | 56.33 |\n",
      "70.00 | 71.33 | 84.67 | 59.67 |\n",
      "72.00 | 75.00 | 84.67 | 61.00 |\n",
      "\n",
      "20.35 | 23.81 | 28.14 | 27.95 |\n",
      "25.54 | 22.94 | 19.91 | 20.35 |\n",
      "22.17 | 19.05 | 31.60 | 32.47 |\n",
      "20.87 | 30.30 | 38.53 | 38.10 |\n",
      "26.09 | 24.68 | 31.60 | 34.63 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_names:\n",
    "    for mode in [\"Direct\", \"CoT\"]:\n",
    "        row = []\n",
    "        for model_name in baseline_results.keys():\n",
    "            a_str = baseline_results[model_name][dataset][mode]['Average_EM_score']\n",
    "            a_str = f\"{100*a_str:.2f}\"\n",
    "            row.append(a_str)\n",
    "        print(\" | \".join(row) + \" |\")\n",
    "    for backup in [\"random\", \"Direct\", \"CoT\"]:\n",
    "        row = []\n",
    "        for model_name in logic_lm_results.keys():\n",
    "            a_str = logic_lm_results[model_name][dataset][backup]['Overall_Accuracy']\n",
    "            a_str = f\"{100*a_str:.2f}\"\n",
    "            row.append(a_str)\n",
    "        print(\" | \".join(row) + \" |\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset          | Prompting                                               | Accuracy (%) for Meta-Llama-3-8B-Instruct | Accuracy (%) for gemini-1.0-pro-vision-001 | Accuracy (%) for gemini-1.5-pro-preview-0409 | Accuracy (%) for gemini-1.5-pro-preview-0514 | Accuracy (%) for gemini-1.5-flash-preview-0514 |\n",
    "| ---------------- | ------------------------------------------------------- | ----------------------------------------- | ------------------------------------------ | -------------------------------------------- | -------------------------------------------- | ----------------------------- |\n",
    "| ProntoQA         | Direct                               | 43                                        |                                            |                                              |                                              |                               |\n",
    "|                  | CoT                               | 76.6                                      |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (random) | 55                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (Direct) | 42.46                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (CoT)    | 8                                         | 92.60                                      | **97.40**                                    | 93.00                                        | 92.60                         |\n",
    "| ProofWriter      | Direct                               | 33                                        |                                            |                                              |                                              |                               |\n",
    "|                  | CoT                               | 28.54                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (random)                       | 28.7                                      |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (Direct) | 28.69                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (CoT)    | 28.695                                    | 74.12                                      | **78.04**                                    | 66.17                                        | 66.17                         |\n",
    "| FOLIO            | Direct                             | 46.5                                      |                                            |                                              |                                              |                               |\n",
    "|                  | CoT                               | 36                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (random)                       | 43                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (Direct) | 53                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (CoT)    | 44.285                                    | 63.50                                      | 75.25                                        | **80.60**                                    | 59.80                         |\n",
    "| LogicalDeduction | Direct                               | 32.33                                     |                                            |                                              |                                              |                               |\n",
    "|                  | CoT                                | 22                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (random)                       | 24.27                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (Direct) | 31                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (CoT)    | 20.38                                     | 64.67                                      | 64.67                                        | **84.67**                                    | 57.67                         |\n",
    "| AR-LSAT          | Direct                              | 7.36                                      |                                            |                                              |                                              |                               |\n",
    "|                  | CoT                               | 8.225                                     |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (random)                       | 22                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (Direct) | 12                                        |                                            |                                              |                                              |                               |\n",
    "|                  | Logic-LM (CoT)    | 6                                         | 24.35                                      | 23.81                                        | 34.20                                        | **35.50**                     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae49cb43b6d9e770d85e5ce15fa5cfe518ebb40f6b4914cb8dc14414c280a84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
