{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ProntoQA|gemini-1.0-pro-vision-001|gemini-1.5-pro-preview-0409|gemini-1.5-pro-preview-0514|gemini-1.5-flash-preview-0514|\n",
      " |---|---|---|---|---| \n",
      "|Overall_Accuracy|0.77|0.96|0.46|0.47|\n",
      "|Executable_Rate|1.00|0.96|0.00|0.00|\n",
      "|Executable_Accuracy|0.77|0.97|0.00|0.00|\n",
      "\n",
      "|ProofWriter|gemini-1.0-pro-vision-001|gemini-1.5-pro-preview-0409|gemini-1.5-pro-preview-0514|gemini-1.5-flash-preview-0514|\n",
      " |---|---|---|---|---| \n",
      "|Overall_Accuracy|0.61|0.75|0.34|0.32|\n",
      "|Executable_Rate|0.64|0.89|0.00|0.05|\n",
      "|Executable_Accuracy|0.76|0.81|0.00|0.54|\n",
      "\n",
      "|FOLIO|gemini-1.0-pro-vision-001|gemini-1.5-pro-preview-0409|gemini-1.5-pro-preview-0514|gemini-1.5-flash-preview-0514|\n",
      " |---|---|---|---|---| \n",
      "|Overall_Accuracy|0.53|0.62|0.71|0.36|\n",
      "|Executable_Rate|0.48|0.59|0.77|0.04|\n",
      "|Executable_Accuracy|0.68|0.82|0.85|1.00|\n",
      "\n",
      "|LogicalDeduction|gemini-1.0-pro-vision-001|gemini-1.5-pro-preview-0409|gemini-1.5-pro-preview-0514|gemini-1.5-flash-preview-0514|\n",
      " |---|---|---|---|---| \n",
      "|Overall_Accuracy|0.61|0.60|0.85|0.56|\n",
      "|Executable_Rate|0.60|0.60|1.00|0.72|\n",
      "|Executable_Accuracy|0.89|0.87|0.85|0.70|\n",
      "\n",
      "|AR-LSAT|gemini-1.0-pro-vision-001|gemini-1.5-pro-preview-0409|gemini-1.5-pro-preview-0514|gemini-1.5-flash-preview-0514|\n",
      " |---|---|---|---|---| \n",
      "|Overall_Accuracy|0.22|0.19|0.32|0.32|\n",
      "|Executable_Rate|0.00|0.00|0.26|0.34|\n",
      "|Executable_Accuracy|0.00|0.00|0.61|0.60|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def generate_markdown_table(logic_lm_results, dataset):\n",
    "    \"\"\"Generates markdown table from JSON data.\n",
    "    \"\"\"\n",
    "    \n",
    "    table_header = f\"|{dataset}|\"\n",
    "    table_header += \"|\".join(logic_lm_results.keys()) \n",
    "    table_header += \"|\\n |---|---|---|---|---| \\n\"\n",
    "\n",
    "    table_rows = []\n",
    "    for metric in ['Overall_Accuracy', 'Executable_Rate', 'Executable_Accuracy']:\n",
    "        row = f\"|{metric}|\"\n",
    "        for model_name in logic_lm_results.keys():\n",
    "            try:\n",
    "                row += f\"{logic_lm_results[model_name][dataset][metric]:.2f}\"\n",
    "            except:\n",
    "                row += \"---\"\n",
    "            row += \"|\"\n",
    "        table_rows.append(row)\n",
    "\n",
    "\n",
    "\n",
    "    table_string = table_header + \"\\n\".join(table_rows)\n",
    "    return table_string\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"./baselines/evaluation/evaluation_baselines.json\", \"r\") as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "with open(\"./outputs/evaluation/evaluation_logic_programs.json\", \"r\") as f:\n",
    "    logic_lm_results = json.load(f)\n",
    "\n",
    "\n",
    "DATASET_NAMES = (\"ProntoQA\", \"ProofWriter\", \"FOLIO\", \"LogicalDeduction\", \"AR-LSAT\")\n",
    "\n",
    "for data_set in DATASET_NAMES:\n",
    "\n",
    "    # Generate markdown table\n",
    "    markdown_table = generate_markdown_table(logic_lm_results, data_set)\n",
    "\n",
    "    # Print markdown table\n",
    "    print(markdown_table)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = (\"ProntoQA\", \"ProofWriter\", \"FOLIO\", \"LogicalDeduction\", \"AR-LSAT\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | gemini-1.0-pro-vision-001 | gemini-1.5-pro-preview-0409 | gemini-1.5-pro-preview-0514 | gemini-1.5-flash-preview-0514 |\n",
    " |---|---|---|---|---| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProntoQA': {'Overall_Accuracy': 0.774,\n",
       "  'Executable_Rate': 1.0,\n",
       "  'Executable_Accuracy': 0.774},\n",
       " 'ProofWriter': {'Overall_Accuracy': 0.6126878130217028,\n",
       "  'Executable_Rate': 0.6444073455759599,\n",
       "  'Executable_Accuracy': 0.7616580310880829},\n",
       " 'AR-LSAT': {'Overall_Accuracy': 0.2217391304347826,\n",
       "  'Executable_Rate': 0.0,\n",
       "  'Executable_Accuracy': 0}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_lm_results['gemini-1.0-pro-vision-001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['gemini-1.5-pro-preview-0409']:\n",
    "    \"\"Generates markdown table from JSON data.\n",
    "    \"\"\"\n",
    "    table_header = \"| Dataset |\"\n",
    "    table_rows = []\n",
    "    for model_name in logic_lm_results.keys():\n",
    "        table_header += f\" {model_name} |\"\n",
    "        table_rows \n",
    "    table_header += \"\\n |---|---|---|---|---| \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall_Accuracy': 0.956,\n",
       " 'Executable_Rate': 0.964,\n",
       " 'Executable_Accuracy': 0.9730290456431535}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_lm_results['gemini-1.5-pro-preview-0409']['ProntoQA']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||ChatGPT (gpt-3.5-turbo)|GPT-3.5 (text-davinci-003)|GPT-4 (gpt-4)|\n",
    "|---|---|---|---|\n",
    "\n",
    "\n",
    "|                 |ChatGPT (gpt-3.5-turbo)|---|---|GPT-3.5 (text-davinci-003)|---|---|GPT-4 (gpt-4)|---|---|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|               |ChatGPT (gpt-3.5-turbo)|GPT-3.5 (text-davinci-003)|GPT-4 (gpt-4)|\n",
    "|----------------|---|---|---|\n",
    "\n",
    "|  | Standard | CoT | Logic-LM | Standard | CoT | Logic-LM | Standard | CoT | Logic-LM |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| ProOntoQA | 47.40 | 67.80 | 61.00 | 51.80 | 83.00 | 85.00 | 77.40 | 98.79 | 83.20 |\n",
    "| ProofWriter | 35.50 | 49.17 | 58.33 | 36.16 | 48.33 | 71.45 | 52.67 | 68.11 | 79.66 |\n",
    "| FOLIO | 45.09 | 57.35 | 62.74 | 54.60 | 57.84 | 61.27 | 69.11 | 70.58 | 78.92 |\n",
    "| LogicalDeduction | 40.00 | 42.33 | 65.67 | 41.33 | 48.33 | 62.00 | 71.33 | 75.25 | 87.63 |\n",
    "| AR-LSAT | 20.34 | 17.31 | 26.41 | 22.51 | 22.51 | 25.54 | 33.33 | 35.06 | 43.04 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_models = [\"gemini-1.0-pro-vision-001\", \"gemini-1.5-pro-preview-0409\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_models = ['gemini-1.5-flash-preview-0514']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"AR-LSAT\"\n",
    "split = \"dev\"\n",
    "model_name = gemini_models[0]\n",
    "backup = \"random\"\n",
    "\n",
    "program_path = os.path.join(\"outputs/logic_programs\", f\"{dataset_name}_{split}_{model_name}.json\")\n",
    "result_path = os.path.join(\"outputs/logic_inference\", f\"{dataset_name}_{split}_{model_name}_backup-{backup}.json\")\n",
    "\n",
    "with open(program_path, \"r\") as json_file:\n",
    "    programs = json.load(json_file)\n",
    "\n",
    "with open(result_path, \"r\") as json_file:\n",
    "    results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m result_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39moutputs/logic_inference\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_backup-\u001b[39m\u001b[39m{\u001b[39;00mbackup\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(program_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[0;32m---> 10\u001b[0m     programs_gpt \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(json_file)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(result_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[1;32m     13\u001b[0m     results_gpt \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(json_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name = \"AR-LSAT\"\n",
    "split = \"dev\"\n",
    "model_name = \"gpt-4\"\n",
    "backup = \"random\"\n",
    "\n",
    "program_path = os.path.join(\"outputs/logic_programs\", f\"{dataset_name}_{split}_{model_name}.json\")\n",
    "result_path = os.path.join(\"outputs/logic_inference\", f\"{dataset_name}_{split}_{model_name}_backup-{backup}.json\")\n",
    "\n",
    "with open(program_path, \"r\") as json_file:\n",
    "    programs_gpt = json.load(json_file)\n",
    "\n",
    "with open(result_path, \"r\") as json_file:\n",
    "    results_gpt = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for program, result, program_gpt, result_gpt in zip(programs, results, programs_gpt, results_gpt):\n",
    "    if result_gpt[\"flag\"] == \"success\" and result[\"flag\"] in [\"parsing error\", \"execution error\"]:\n",
    "        print(program[\"raw_logic_programs\"][0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for program, result in zip(programs, results):\n",
    "    # if result[\"id\"] ==  \"ar_lsat_200006_1-G_1_6\":\n",
    "    #     print(result[\"flag\"])\n",
    "    # print(result[\"flag\"])\n",
    "    if result[\"flag\"] == \"success\":\n",
    "        print(result[\"flag\"])\n",
    "        print(program[\"raw_logic_programs\"][0])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.symbolic_solvers.z3_solver.sat_problem_solver import LSAT_Z3_Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(D)']\n"
     ]
    }
   ],
   "source": [
    "logic_program = '''# Declarations\n",
    "people = EnumSort([Vladimir, Wendy])\n",
    "meals = EnumSort([breakfast, lunch, dinner, snack])\n",
    "foods = EnumSort([fish, hot_cakes, macaroni, omelet, poached_eggs])\n",
    "eats = Function([people, meals] -> [foods])\n",
    "\n",
    "# Constraints\n",
    "ForAll([m:meals], eats(Vladimir, m) != eats(Wendy, m)) ::: At no meal does Vladimir eat the same kind of food as Wendy\n",
    "ForAll([p:people, f:foods], Count([m:meals], eats(p, m) == f) <= 1) ::: Neither of them eats the same kind of food more than once during the day\n",
    "ForAll([p:people], Or(eats(p, breakfast) == hot_cakes, eats(p, breakfast) == poached_eggs, eats(p, breakfast) == omelet)) ::: For breakfast, each eats exactly one of the following: hot cakes, poached eggs, or omelet\n",
    "ForAll([p:people], Or(eats(p, lunch) == fish, eats(p, lunch) == hot_cakes, eats(p, lunch) == macaroni, eats(p, lunch) == omelet)) ::: For lunch, each eats exactly one of the following: fish, hot cakes, macaroni, or omelet\n",
    "ForAll([p:people], Or(eats(p, dinner) == fish, eats(p, dinner) == hot_cakes, eats(p, dinner) == macaroni, eats(p, dinner) == omelet)) ::: For dinner, each eats exactly one of the following: fish, hot cakes, macaroni, or omelet\n",
    "ForAll([p:people], Or(eats(p, snack) == fish, eats(p, snack) == omelet)) ::: For a snack, each eats exactly one of the following: fish or omelet\n",
    "eats(Wendy, lunch) == omelet ::: Wendy eats an omelet for lunch\n",
    "\n",
    "# Options\n",
    "Question ::: Vladimir must eat which one of the following foods?\n",
    "is_valid(Exists([m:meals], eats(Vladimir, m) == fish)) ::: (A)\n",
    "is_valid(Exists([m:meals], eats(Vladimir, m) == hot_cakes)) ::: (B)\n",
    "is_valid(Exists([m:meals], eats(Vladimir, m) == macaroni)) ::: (C)\n",
    "is_valid(Exists([m:meals], eats(Vladimir, m) == omelet)) ::: (D)\n",
    "is_valid(Exists([m:meals], eats(Vladimir, m) == poached_eggs)) ::: (E)'''\n",
    "\n",
    "\n",
    "z3_program = LSAT_Z3_Program(logic_program, 'AR-LSAT')\n",
    "# print(z3_program.standard_code)\n",
    "\n",
    "output, error_message = z3_program.execute_program()\n",
    "print(output)\n",
    "# print(z3_program.answer_mapping(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae49cb43b6d9e770d85e5ce15fa5cfe518ebb40f6b4914cb8dc14414c280a84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
